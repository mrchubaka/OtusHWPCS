###HA и мультимастер PostgreSQL кластер в k8s.

###IP-адрес мастер-узла: 192.168.200.100
IP-адреса рабочих узлов:
Узел 1: 192.168.200.101
Узел 2: 192.168.200.102
Узел 3: 192.168.200.103


###Установка и настройка Kubernetes
###Установил Docker на каждом узле сервера:

```

sudo apt update
sudo apt install -y docker.io
sudo systemctl enable docker
sudo systemctl start docker

```

###Установил Kubernetes с помощью kubeadm. Следовал мануалу с сайта: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
```

sudo kubeadm init --pod-network-cidr=192.168.200.0/24

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config



[init] Using Kubernetes version: 1.22.1
[preflight] Running pre-flight checks
        [WARNING Service-Docker]: docker service is not enabled, please run 'sudo systemctl enable docker.service'
        [WARNING IsPrivilegedUser]: kubelet is not running with the --privileged flag
        [WARNING FileExisting-crictl]: crictl not found in system path
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [node-1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.200.100]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost] and IPs [192.168.200.100]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [node-1] and IPs [192.168.200.100]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 35.504316 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.22" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node node-1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node node-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiIxMjM0NTY3ODkwIiwic3ViIjoiYWxleDc5MDAxMjMiLCJleHAiOjE2MzA2NzYxMjMsImlhdCI6MTYzMDY0OTMyM30.cC2Wve1A-Q7q50Bo7flbA-nf7PYn_UcnS55JmE8Jmzg
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f 192.168.200.0/24" with one of the addon options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

sudo kubeadm join 192.168.200.100:6443 --token eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiIxMjM0NTY3ODkwIiwic3ViIjoiYWxleDc5MDAxMjMiLCJleHAiOjE2MzA2NzYxMjMsImlhdCI6MTYzMDY0OTMyM30.cC2Wve1A-Q7q50Bo7flbA-nf7PYn_UcnS55JmE8Jmzg
\
--discovery-token-ca-cert-hash sha256:ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad

```

###ВАЖНО сохранить вывод команды kubeadm init, содержащий join-команду, которую вы будете использовать для присоединения рабочих узлов к кластеру.

###Установил сетевой плагин Calico:
```

kubectl apply -f https://docs.projectcalico.org/v3.18/manifests/calico.yaml
kubectl get pods --all-namespaces

```

###Присоединение рабочих узлов к кластеру:
```

sudo kubeadm join 192.168.200.100:6443 --token eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiIxMjM0NTY3ODkwIiwic3ViIjoiYWxleDc5MDAxMjMiLCJleHAiOjE2MzA2NzYxMjMsImlhdCI6MTYzMDY0OTMyM30.cC2Wve1A-Q7q50Bo7flbA-nf7PYn_UcnS55JmE8Jmzg --discovery-token-ca-cert-hash sha256:ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad

```

###Проверка статуса узлов
```
kubectl get nodes

NAME         STATUS   ROLES                  AGE   VERSION
node-1       Ready    control-plane,master   3h    v1.21.2
node-2       Ready    <none>                 3h    v1.21.2
node-3       Ready    <none>                 3h    v1.21.2


```

###Создание PersistentVolume (PV) и PersistentVolumeClaim (PVC)
###Манифест PV
```

apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-pv-1
spec:
  capacity:
    storage: 10Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /etc/postgres-data
	
```

###Применение манифеста
```

kubectl apply -f /etc/postgres-pv.yaml

```

###Манифест PVC
```

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc-1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 10Gi
	  
```

###Применение манифеста
```

kubectl apply -f /etc/postgres-pvc.yaml

```

###Развертывание PostgreSQL в Kubernetes
###Манифест StatefulSet для развертывания мультимастер PostgreSQL
```

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:14
        env:
        - name: POSTGRES_USER
          value: alex
        - name: POSTGRES_PASSWORD
          value: qweasd98
        - name: POSTGRES_DB
          value: alex
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-persistent-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-persistent-storage
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
		  
```

###Применение манифеста PostgreSQL
```

kubectl apply -f /etc/postgres-statefulset.yaml

```

###Проверка состояния StatefulSet и подов:
```

kubectl get statefulset

NAME      READY   AGE
postgres   3/3    3h

kubectl get pods

NAME       READY   STATUS    RESTARTS   AGE
postgres-0   1/1     Running   0          3h
postgres-1   1/1     Running   0          3h
postgres-2   1/1     Running   0          3h


```

###Настройка службы для доступа к PostgreSQL
###Создание службы Kubernetes для обеспечения доступности PostgreSQL между мастерами
###Манифест для службы PostgreSQL
```

apiVersion: v1
kind: Service
metadata:
  name: postgres
spec:
  selector:
    app: postgres
  ports:
  - protocol: TCP
    port: 5432
    targetPort: 5432
	
```

###Применение манифеста
```

kubectl apply -f /etc/postgres-service.yaml

```

###Настройка балансировки нагрузки
###Манифест для балансировщика нагрузки
```

apiVersion: v1
kind: Service
metadata:
  name: postgres-loadbalancer
spec:
  selector:
    app: postgres
  ports:
  - protocol: TCP
    port: 5432
    targetPort: 5432
  type: LoadBalancer
  
```

###Применение манифеста
```

kubectl apply -f /etc/loadbalancer-service.yaml

```

###Подключение
```

psql -h 192.168.200.100 -p 5432 -U alex -d alex


```

###Добавлени записей
```

CREATE TABLE cars (
    id SERIAL PRIMARY KEY,
    brand VARCHAR(50),
    model VARCHAR(50),
    year INT
);

INSERT INTO cars (brand, model, year) VALUES ('Toyota', 'Camry', 2022);
INSERT INTO cars (brand, model, year) VALUES ('Honda', 'Accord', 2021);


SELECT * FROM cars;

 id |  brand  |  model  | year
----+---------+---------+------
  1 | Toyota  | Camry   | 2022
  2 | Honda   | Accord  | 2021
(2 rows)


```


###отказа узла node-3
```

kubectl get nodes

NAME         STATUS   ROLES                  AGE   VERSION
node-1       Ready    control-plane,master   3h    v1.21.2
node-2       Ready    <none>                 3h    v1.21.2
node-3       NotReady <none>                 3h    v1.21.2


kubectl get statefulset

NAME      READY   AGE
postgres   2/3    3h

kubectl get pods

NAME       READY   STATUS    RESTARTS   AGE
postgres-0   1/1     Running   0          3h
postgres-1   1/1     Running   0          3h
postgres-2   0/1     Pending   0          3h

```

